{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://github.com/pzaino/thecrowler/main/schemas/crowler-config-schema.json",
  "title": "CROWler Configuration Schema",
  "description": "This is the configuration schema for the CROWler. This schema describes all the sections of the config.yaml configuration file and provides info on configuring a fleet of CROWler's Engines, VDIs, APIs and how they should work. Keep in mind that the 'remote' property should be used on its own, and the 'database', 'crawler', 'api', 'selenium', 'network_info' properties should be used together.",
  "type": "object",
  "properties": {
    "version": {
      "title": "CROWler Configuration version",
      "description": "This is the version of the CROWler configuration. This is for you to version your work.",
      "type": "string",
      "pattern": "^\\d+\\.\\d+\\.\\d+$"
    },
    "author": {
      "title": "CROWler Configuration author",
      "description": "This is the author of the CROWler configuration.",
      "type": "string"
    },
    "description": {
      "title": "CROWler Configuration description",
      "description": "A description field.",
      "type": "string"
    },
    "created_at": {
      "title": "CROWler Configuration creation date",
      "description": "This is the date when the CROWler configuration was created.",
      "type": "string",
      "pattern": "(?:(?:(?:(\\d{4})[-\\/\\.](\\d{2})[-\\/\\.](\\d{2}))|(?:(\\d{2})[-\\/\\.](\\d{2})[-\\/\\.](\\d{4})))\\s*(?:T\\s*)?)?(?:(\\d{1,2}):(\\d{2})(?::(\\d{2}))?\\s*([AaPp][Mm])?)?"
    },
    "remote": {
      "title": "Remote Configuration",
      "description": "This is the remote configuration section to tell the CROWler's Engine that the actual config.yaml configuration has to be fetched remotely from a distribution server. If you use this section, then do not populate the other configuration sections as they will be ignored. The CROWler will fetch its configuration from the remote server and use it to start the engine.",
      "type": "object",
      "properties": {
        "host": {
          "title": "CROWler Configuration Distribution Server Host",
          "description": "This is the host that the CROWler will use to fetch its configuration.",
          "type": "string",
          "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
        },
        "path": {
          "title": "CROWler Configuration Distribution Server Path",
          "description": "This is the path that the CROWler will use to fetch its configuration.",
          "type": "string"
        },
        "port": {
          "title": "CROWler Configuration Distribution Server Port",
          "description": "This is the port that the CROWler will use to fetch its configuration.",
          "type": "string",
          "examples": [
            "8086"
          ]
        },
        "region": {
          "title": "CROWler Configuration Distribution Server Region",
          "description": "This is the region that the CROWler will use to fetch its configuration. For example in case the distribution server is on an AWS S3 bucket, you can specify the region here.",
          "type": "string"
        },
        "token": {
          "title": "CROWler Configuration Distribution Server Token",
          "description": "This is the token that the CROWler will use to connect to the distribution server to fetch its configuration.",
          "type": "string"
        },
        "secret": {
          "title": "CROWler Configuration Distribution Server Secret",
          "description": "This is the secret that the CROWler will use to connect to the distribution server to fetch its configuration.",
          "type": "string"
        },
        "timeout": {
          "title": "CROWler Configuration Distribution Server Timeout",
          "description": "This is the timeout for the CROWler to fetch its configuration.",
          "type": "integer",
          "minimum": 10
        },
        "type": {
          "title": "CROWler Configuration Distribution Server Type",
          "description": "This is the type of the distribution server that the CROWler will use to fetch its configuration. For example, s3 or http.",
          "type": "string",
          "enum": [
            "s3",
            "http",
            "local",
            "ftp",
            ""
          ]
        },
        "sslmode": {
          "title": "CROWler Configuration Distribution Server SSL Mode",
          "description": "This is the sslmode that the CROWler will use to connect to the distribution server to fetch its configuration.",
          "type": "string",
          "enum": [
            "enable",
            "disable",
            ""
          ],
          "examples": [
            "enable",
            "disable"
          ]
        }
      },
      "additionalProperties": false,
      "required": [
        "host",
        "path",
        "type"
      ]
    },
    "database": {
      "title": "CROWler DB Configuration CDBI",
      "description": "This is the database configuration section, it's used to tell the CROWler how to connect to the database to store collected data, which type of database we use and other options to optimize the database for a specific use case.",
      "type": "object",
      "properties": {
        "type": {
          "title": "CROWler DB Type",
          "description": "This is the type of the database that the CROWler will use to store data. For example, postgres.",
          "type": "string",
          "enum": [
            "postgres",
            "mysql",
            "sqlite"
          ]
        },
        "host": {
          "title": "CROWler DB Host",
          "description": "This is the host that the CROWler will use to connect to the database.",
          "type": "string",
          "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
        },
        "port": {
          "title": "CROWler DB Port",
          "description": "This is the port that the CROWler will use to connect to the database.",
          "type": "integer",
          "minimum": 1,
          "maximum": 65535,
          "examples": [
            5432
          ]
        },
        "user": {
          "title": "CROWler DB User",
          "description": "This is the user that the CROWler will use to connect to the database.",
          "type": "string"
        },
        "password": {
          "title": "CROWler DB Password",
          "type": "string"
        },
        "dbname": {
          "title": "CROWler DB Name",
          "description": "This is the name of the database that the CROWler will use to store data.",
          "type": "string",
          "examples": [
            "SitesIndex"
          ]
        },
        "retry_time": {
          "title": "CROWler DB Retry Time",
          "description": "This is the time in seconds that the CROWler will wait before retrying a database connection.",
          "type": "integer",
          "minimum": 5,
          "examples": [
            10
          ]
        },
        "ping_time": {
          "title": "CROWler DB Ping Time",
          "description": "This is the time in seconds that the CROWler will wait before pinging the database to check if it is still alive.",
          "type": "integer",
          "minimum": 5,
          "examples": [
            10
          ]
        },
        "sslmode": {
          "title": "CROWler DB SSL Mode",
          "description": "This is the sslmode that the CROWler will use to connect to the database. Use 'enable' to enable the ssl mode connection to the DB. (default is 'disable').",
          "type": "string",
          "enum": [
            "enable",
            "disable",
            ""
          ],
          "examples": [
            "enable",
            "disable"
          ]
        },
        "optimize_for": {
          "title": "CROWler DB Optimize For",
          "description": "This option allows the user to optimize the database for a specific use case. For example, if the user is doing more write operations than query, then use the value 'write'. If the user is doing more query operations than write, then use the value 'query'. If unsure leave it empty.",
          "type": "string",
          "enum": [
            "write",
            "query",
            "none",
            ""
          ],
          "examples": [
            "write",
            "query"
          ]
        },
        "max_conns": {
          "title": "CROWler DB Max Connections",
          "description": "This is the maximum number of connections that the CROWler will use to connect to the database.",
          "type": "integer",
          "minimum": 25,
          "examples": [
            100
          ]
        },
        "max_idle_conns": {
          "title": "CROWler DB Max Idle Connections",
          "description": "This is the maximum number of idle connections that the CROWler will use to connect to the database. Suggestion, keep the number of idle connections to 25% / 30% of the max connections, unless you have plenty of resources.",
          "type": "integer",
          "minimum": 25,
          "examples": [
            50
          ]
        }
      },
      "additionalProperties": false,
      "required": [
        "type",
        "host",
        "user",
        "password",
        "dbname"
      ]
    },
    "crawler": {
      "title": "CROWler Engine's crawling Configuration CEI",
      "description": "This is the crawler (CROWler Engine) configuration section, it's used to tell the CROWler's engine how to behave. It is the configuration for the CROWler engine that the CROWler will use to crawl websites, spin workers and configure it's internal Control API.",
      "type": "object",
      "properties": {
        "query_timer": {
          "title": "CROWler Engine Query Timer",
          "description": "This is the query timer for the CROWler Engine. It is the time in seconds that the CROWler Engine will wait before querying the database for new sources to crawl. (default every 30 seconds)",
          "type": "integer",
          "minimum": 1,
          "examples": [
            5,
            15
          ]
        },
        "workers": {
          "title": "CROWler Engine Workers",
          "description": "This is the number of workers that the CROWler Engine will use to crawl websites. Minimum number is 3 per each Source if you have network discovery enabled or 1 per each source if you are doing crawling only. Increase the number of workers to scale up the CROWler engine vertically.",
          "type": "integer",
          "minimum": 3,
          "examples": [
            5,
            10
          ]
        },
        "engine": {
          "title": "CROWler Engine Configuration for each CROWler Engine instance",
          "description": "This is the configuration for each CROWler Engine instance. This section is used to customize the CROWler Engine behavior for each instance.",
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "name": {
                "title": "CROWler Engine Instance Name",
                "description": "This is the name of the CROWler Engine instance. Normally it's either the hostname or the container name in which the engine is running.",
                "type": "string",
                "examples": [
                  "crowler-engine-1",
                  "crowler-engine-2"
                ]
              },
              "query_timer": {
                "title": "CROWler Engine Query Timer",
                "description": "This is the query timer for the CROWler Engine. It is the time in seconds that the CROWler Engine will wait before querying the database for new sources to crawl.",
                "type": "integer",
                "minimum": 1,
                "examples": [
                  5,
                  15
                ]
              },
              "source_priority": {
                "title": "Configure the priority that this instance will use to filter the sources",
                "description": "This is a list of priorities that CROWler Engine will use to filter the sources. For example 'high' to filter for high priority, 'medium,low' to pick only medium and low priority sources, 'my_priority' to pick the source with your specific custom priority.",
                "type": "array",
                "items": {
                  "type": "string",
                  "examples": [
                    "high",
                    "medium",
                    "low",
                    "my_priority"
                  ]
                }
              },
              "vdi_name": {
                "title": "CROWler Engine VDI Name",
                "description": "Specify a list of VDIs that this instance of the Engine will use to crawl sources that have been picked by it.",
                "type": "array",
                "items": {
                  "type": "string",
                  "examples": [
                    "crowler-vdi-1",
                    "crowler-vdi-japan-1"
                  ]
                }
              },
              "schedule": {
                "title": "CROWler Engine Working Schedule",
                "description": "Defines when this CROWler Engine instance should operate. Supports either a date range or recurring weekdays, and a daily time range.",
                "type": "object",
                "properties": {
                  "timezone": {
                    "title": "Timezone",
                    "description": "IANA timezone identifier used for schedule evaluation (e.g., 'Europe/London', 'America/New_York'). If omitted, system timezone is used.",
                    "type": "string",
                    "pattern": "^[A-Za-z_]+(?:/[A-Za-z_]+)+$",
                    "examples": [
                      "Europe/London",
                      "America/New_York",
                      "Asia/Tokyo"
                    ]
                  },
                  "active_dates": {
                    "title": "Active Date Range",
                    "description": "Defines the date range when the engine is allowed to run. If omitted, the engine is considered always active.",
                    "type": "object",
                    "properties": {
                      "from": {
                        "type": "string",
                        "format": "date-time",
                        "description": "Start date and time for activation (ISO8601).",
                        "examples": [
                          "2025-11-06T09:00:00Z"
                        ]
                      },
                      "to": {
                        "type": "string",
                        "format": "date-time",
                        "description": "End date and time for deactivation (ISO8601).",
                        "examples": [
                          "2025-12-01T18:00:00Z"
                        ]
                      }
                    },
                    "required": [
                      "from",
                      "to"
                    ],
                    "additionalProperties": false
                  },
                  "weekdays": {
                    "title": "Active Weekdays",
                    "description": "Defines which days of the week the engine can operate. If omitted, the engine can run on any day.",
                    "type": "array",
                    "items": {
                      "type": "string",
                      "enum": [
                        "monday",
                        "tuesday",
                        "wednesday",
                        "thursday",
                        "friday",
                        "saturday",
                        "sunday"
                      ]
                    },
                    "examples": [
                      [
                        "monday",
                        "tuesday",
                        "wednesday",
                        "thursday",
                        "friday"
                      ]
                    ]
                  },
                  "time_range": {
                    "title": "Active Time Range",
                    "description": "Defines the time window (in 24-hour HH:MM format) when the engine can operate during active days.",
                    "type": "object",
                    "properties": {
                      "from": {
                        "type": "string",
                        "pattern": "^(?:[01]\\d|2[0-3]):[0-5]\\d$",
                        "description": "Start time of the working window.",
                        "examples": [
                          "08:00",
                          "09:30"
                        ]
                      },
                      "to": {
                        "type": "string",
                        "pattern": "^(?:[01]\\d|2[0-3]):[0-5]\\d$",
                        "description": "End time of the working window.",
                        "examples": [
                          "17:00",
                          "18:00"
                        ]
                      }
                    },
                    "required": [
                      "from",
                      "to"
                    ],
                    "additionalProperties": false
                  }
                },
                "additionalProperties": false
              }
            }
          },
          "examples": [
            {
              "priority": "high",
              "vdi_name": "vdi1,vdi2"
            },
            {
              "priority": "medium,low",
              "vdi_name": "vdi3"
            }
          ]
        },
        "platform": {
          "title": "CROWler Engine Platform",
          "description": "This is the platform that the CROWler Engine will use to crawl websites. For example, 'desktop' or 'mobile' (mobile is experimental!).",
          "type": "string",
          "enum": [
            "desktop",
            "mobile",
            ""
          ],
          "examples": [
            "desktop",
            "mobile"
          ]
        },
        "browser_platform": {
          "title": "CROWler Engine Browser Platform",
          "description": "This is the browser platform id. The CROWler Engine will use it to select the user-agent string to use to configure the session. For example, 'darwin', 'window', 'random' or 'linux'.",
          "type": "string",
          "enum": [
            "darwin",
            "windows",
            "linux",
            "random",
            ""
          ],
          "examples": [
            "darwin",
            "windows",
            "linux"
          ]
        },
        "interval": {
          "title": "CROWler Engine Page Rendering Interval",
          "description": "This is the interval at which the CROWler Engine will crawl websites. It is the part of the HBS, values are in seconds, e.g. '3' means 3 seconds. For the interval you can also use the CROWler exprterpreter to generate delay values at runtime, e.g., 'random(1, 3)' or 'random(random(1,3), random(5,8))'.",
          "type": "string",
          "examples": [
            "3",
            "random(1, 3)",
            "random(random(1,3), random(5,8))"
          ]
        },
        "timeout": {
          "title": "CROWler Engine Page Fetching Timeout",
          "description": "This is the timeout (in seconds) for the CROWler Engine. It is the maximum amount of time that the CROWler Engine will wait for a website to respond.",
          "type": "integer",
          "minimum": 5,
          "examples": [
            30
          ]
        },
        "crawling_interval": {
          "title": "CROWler Engine Crawling Interval",
          "description": "This is the interval at which the CROWler Engine will crawl websites. It is the interval at which the CROWler will crawl each given source. The default value is '3 days', e.g. '1 day' means crawl each source every day.",
          "type": "string",
          "examples": [
            "3 days",
            "1 hour",
            "1 day",
            "3 hours"
          ]
        },
        "crawling_if_error": {
          "title": "CROWler Engine Crawling If Error",
          "description": "This is re-crawling-after in case of error during the original crawling. Default is 'retry after 15 minutes'. For example, '1 hour' means re-crawl after 1 hour.",
          "type": "string",
          "examples": [
            "15 minutes",
            "1 hour",
            "1 day"
          ]
        },
        "crawling_if_ok": {
          "title": "CROWler Engine Crawling If OK",
          "description": "This is re-crawling-after in case of success during the original crawling. Default is 'retry after 3 days'. For example, '1 hour' means re-crawl after 1 hour.",
          "type": "string",
          "examples": [
            "3 days",
            "1 hour",
            "1 day"
          ]
        },
        "processing_timeout": {
          "title": "CROWler Engine Processing Timeout",
          "description": "This is the timeout for the CROWler Engine to process a website. It is the maximum amount of time that the CROWler will wait for a website to be processed. If a source stays in processing for more than this time, the CROWler will re-schedule it for a new processing.",
          "type": "string",
          "examples": [
            "1 day",
            "3 days"
          ]
        },
        "maintenance": {
          "title": "CROWler Engine DB Maintenance Interval",
          "description": "This is the DB maintenance interval (in seconds) for the CROWler Engine. It is the interval at which the CROWler will perform automatic maintenance tasks, a value of 0 means NO automatic DB maintenance.",
          "type": "integer",
          "minimum": 0,
          "examples": [
            0,
            3600
          ]
        },
        "source_screenshot": {
          "title": "CROWler Engine Source Screenshot",
          "description": "This is a flag that tells the CROWler to take a screenshot of the source website. This is useful for debugging purposes.",
          "type": "boolean"
        },
        "full_site_screenshot": {
          "title": "CROWler Engine Full Site Screenshots",
          "description": "This is a flag that tells the CROWler to take a screenshot of the full website. This is useful for debugging purposes.",
          "type": "boolean"
        },
        "max_depth": {
          "title": "CROWler Engine Crawling Maximum Depth",
          "description": "This is the maximum depth that the CROWler Engine will crawl websites.",
          "type": "integer",
          "minimum": 1,
          "examples": [
            3,
            5
          ]
        },
        "max_links": {
          "title": "CROWler Engine Crawling Maximum Number of Links",
          "description": "This is the maximum number of links that the CROWler Engine will crawl per each Source. If zero, no limit.",
          "type": "integer",
          "minimum": 1,
          "examples": [
            3,
            5
          ]
        },
        "max_sources": {
          "title": "CROWler Engine Maximum Sources",
          "description": "This is the maximum number of sources that a single instance of the CROWler's engine will fetch atomically and atomically to enqueue in the jobs-queue and crawl.",
          "type": "integer",
          "minimum": 1,
          "examples": [
            4,
            10,
            20
          ]
        },
        "initial_ramp_up": {
          "title": "CROWler Engine Initial Ramp Up",
          "description": "This is the initial ramp up for the CROWler Engine. It is the time in seconds that the CROWler Engine will wait in between starting collecting data for each source in a new batch (at the beginning of a new collection). Valid values are < 0 for automatic ramp-up, 0 for no ramp-up, > 0 for manual ramp-up. A ramp-up can be useful to help proxies to 'warm up'.",
          "type": "integer",
          "examples": [
            -1,
            5,
            15
          ]
        },
        "delay": {
          "title": "CROWler Engine Delay Between Page's Fetching Requests",
          "description": "This is the delay between requests that the CROWler Engine will use to crawl websites. It is the delay between requests that the CROWler will use as part of the HBS. For delay you can also use the CROWler exprterpreter to generate delay values at runtime, e.g., 'random(1, 3)' or 'random(random(1,3), random(5,8))'.",
          "type": "string",
          "examples": [
            "3",
            "random(1, 3)",
            "random(random(1,3), random(5,8))"
          ]
        },
        "browsing_mode": {
          "title": "CROWler Engine Browsing Mode",
          "description": "This is the 'default' browsing mode that the CROWler Engine will use to crawl websites. For example, recursive, human, or fuzzing.\n- default or empty string means use recursive mode.\n- recursive means the CROWler will crawl websites in a recursive way.\n- right_click_recursive means the CROWler will crawl websites in a right-click recursive way.\n- human means the CROWler will crawl websites in a human way.\n- fuzzing means the CROWler will crawl websites by fuzzing URL and Query Parameters (this also requires crawling rules!).",
          "type": "string",
          "enum": [
            "default",
            "recursive",
            "right_click_recursive",
            "human",
            "fuzzing",
            ""
          ],
          "examples": [
            "default",
            "recursive",
            "human",
            "fuzzing"
          ]
        },
        "max_retries": {
          "title": "CROWler Engine Maximum Retries for a Website",
          "description": "This is the maximum number of times that the CROWler Engine will retry a request to a website. If the CROWler is unable to fetch a website after this number of retries, it will move on to the next website.",
          "type": "integer",
          "minimum": 0,
          "examples": [
            3,
            5
          ]
        },
        "max_requests": {
          "title": "CROWler Engine Maximum Requests for a Website",
          "description": "This is the maximum number of requests that the CROWler will send to a website. If the CROWler sends this number of requests to a website and is unable to fetch the website, it will move on to the next website. A value of 0 means no limit.",
          "type": "integer",
          "minimum": 0,
          "examples": [
            3,
            1000
          ]
        },
        "change_useragent": {
          "title": "CROWler Engine Change UserAgent at every new request",
          "description": "This is the policy that the CROWler Engine will use to change the User-Agent at every new request. For example, 'always' means the CROWler will change the User-Agent on every request, 'never' means the CROWler will never change the User-Agent, 'on_start' means the CROWler will change the User-Agent only at the beginning of a crawling process.",
          "type": "string",
          "enum": [
            "always",
            "never",
            "on_start",
            ""
          ]
        },
        "force_sec_fetch_site_same_origin": {
          "title": "CRWOler engine forces sec-fetch-site same-origin",
          "description": "This is a technique that is required for websites that requires sec-fetch-site same-origin for all pages except the home page. Some website uses this technique to block crawlers from crawl specific URLs. The CROWler can go around it when the user enable this check.",
          "type": "boolean"
        },
        "reset_cookies_policy": {
          "title": "CROWler Engine Reset Cookies Policy",
          "description": "This is the policy that the CROWler Engine will use to reset cookies. For example, 'always' means the CROWler will reset cookies on every request and when done, 'never' means the CROWler will never reset cookies, 'on_start' means the CROWler will reset cookies only at the beginning of a crawling process.",
          "type": "string",
          "enum": [
            "always",
            "never",
            "none",
            "on_request",
            "on_start",
            "when_done",
            ""
          ],
          "examples": [
            "always",
            "never",
            "on_request"
          ]
        },
        "no_third_party_cookies": {
          "title": "CROWler Engine No Third Party Cookies",
          "description": "This is a flag that tells the CROWler Engine to not allow third-party cookies. This is useful for privacy reasons.",
          "type": "boolean"
        },
        "request_images": {
          "title": "CROWler Engine Request Images",
          "description": "This is a flag that tells the CROWler to request images from a website. This can be useful to reduce bandwidth usage (if set to false) and speed up the crawling process. Images are requested by default.",
          "type": "boolean"
        },
        "request_css": {
          "title": "CROWler Engine Request CSS",
          "description": "This is a flag that tells the CROWler to request CSS from a website. This can be useful to reduce bandwidth usage (if set to false) and speed up the crawling process. CSS are requested by default.",
          "type": "boolean"
        },
        "request_scripts": {
          "title": "CROWler Engine Request Scripts",
          "description": "This is a flag that tells the CROWler to request scripts from a website. This can be useful to reduce bandwidth usage (if set to false) and speed up the crawling process, but can break a site functioning (so make sure you know what you're doing when you set it to false). Scripts are requested by default.",
          "type": "boolean"
        },
        "request_plugins": {
          "title": "CROWler Engine Request Plugins",
          "description": "This is a flag that tells the CROWler to request plugins from a website. This can be useful to reduce bandwidth usage (if set to false) and speed up the crawling process, but can break a site functioning (so make sure you know what you're doing when you set it to false). Plugins are requested by default.",
          "type": "boolean"
        },
        "request_frames": {
          "title": "CROWler Engine Request Frames",
          "description": "This is a flag that tells the CROWler to request frames from a website. This can be useful to reduce bandwidth usage (if set to false) and speed up the crawling process, but can break a site functioning (so make sure you know what you're doing when you set it to false). Frames are requested by default.",
          "type": "boolean"
        },
        "prevent_duplicate_urls": {
          "title": "CROWler Engine Prevent Duplicate URLs",
          "description": "This is a flag that tells the CROWler to prevent crawling of duplicate URLs. This can be useful to reduce redundant work and save resources. It's a tag that is not reliable 100%, but it can help to reduce the number of duplicate URLs that the CROWler will crawl.",
          "type": "boolean"
        },
        "refresh_content": {
          "title": "CROWler Engine Refresh Content",
          "description": "This is a flag that tells the CROWler to refresh data for each page, not create multiple collection's copies. When enabled, the CROWler will delete the current data for a page in the DB and recollect anew. This is useful to create solutions that only store 'current' data vs solutions that need 'historical data' (e.g., for change detection).",
          "type": "boolean"
        },
        "collect_html": {
          "title": "CROWler Engine Collect Page's HTML",
          "description": "This is a flag that tells the CROWler to collect the HTML of a website. This is also useful for debugging purposes. This collection is automatic and for each page of a Source.",
          "type": "boolean"
        },
        "collect_images": {
          "title": "CROWler Engine Collect Page's Images",
          "description": "This is a flag that tells the CROWler to collect images from a website. This is also useful for debugging purposes. This collection is automatic and for each page of a Source",
          "type": "boolean"
        },
        "collect_files": {
          "title": "CROWler Engine Collect Page's Files",
          "description": "This is a flag that tells the CROWler to collect files from a website. This is also useful for debugging purposes. This collection is automatic and for each page of a Source",
          "type": "boolean"
        },
        "collect_content": {
          "title": "CROWler Engine Collect Page's Content (as text)",
          "description": "This is a flag that tells the CROWler to collect the text content of a website. This is also useful for AI datasets creation and knowledge bases. This collection is automatic and for each page of a Source",
          "type": "boolean"
        },
        "collect_keywords": {
          "title": "CROWler Engine Collect Page's Keywords",
          "description": "This is a flag that tells the CROWler to collect the keywords of a website. This is also useful for AI datasets creation and knowledge bases. This collection is automatic and for each page of a Source. Keywords and metadata are used in searches, so we recommend enabling this option.",
          "type": "boolean"
        },
        "collect_metatags": {
          "title": "CROWler Engine Collect Page's Metatags",
          "description": "This is a flag that tells the CROWler to collect the metatags of a website. This is useful for AI datasets creation and knowledge bases. This collection is automatic and for each page of a Source. Keywords and metadata are used in searches, so we recommend enabling this option.",
          "type": "boolean"
        },
        "collect_performance": {
          "title": "CROWler Engine Collect Page's Performance",
          "description": "This is a flag that tells the CROWler to collect the performance of each page of a website.",
          "type": "boolean"
        },
        "collect_events": {
          "title": "CROWler Engine Collect Page's Events",
          "description": "This is a flag that tells the CROWler to collect the events of a website. This is useful for Cybersecurity applications, given it collects all page's events, included JavaScript events like calling-back home etc. This collection is automatic and for each page of a Source.",
          "type": "boolean"
        },
        "collect_xhr": {
          "title": "CROWler Engine Collect Page's XHR",
          "description": "This is a flag that tells the CROWler to collect the XHR of a website. This is useful for Cybersecurity applications, given it collects all page's XHR requests. This collection is automatic and for each page of a Source.",
          "type": "boolean"
        },
        "filter_xhr": {
          "title": "CROWler Engine Filter Page's XHR",
          "description": "This is a flag that tells the CROWler to filter the XHR of a website. This is useful for Cybersecurity applications, given it filters all page's XHR requests. This collection is automatic and for each page of a Source.",
          "type": "array",
          "items": {
            "type": "string",
            "examples": [
              "text/html",
              "application/javascript",
              "image/png"
            ]
          }
        },
        "collect_links": {
          "title": "CROWler Engine Collect Page's Links",
          "description": "This is a flag that tells the CROWler to collect the links of a website. This is useful for AI datasets creation and knowledge bases. This collection is automatic and for each page of a Source.",
          "type": "boolean"
        },
        "create_event_when_done": {
          "title": "CROWler Engine Create Event When Done",
          "description": "This is a flag that tells the CROWler to create an event when the crawling process is done. The event will be created with the event type `crawl_completed`. This is useful for monitoring purposes.",
          "type": "boolean"
        },
        "control": {
          "title": "CROWler Engine (internal) Control API Configuration",
          "description": "This is the CROWler's Control API configuration. The Control API is an internal management API that resides within the CROWler Engine. Its primary purpose is to allow internal tools, like health checks, to monitor and manage the operational status of the CROWler Engine (e.g., starting, stopping, or checking the status of crawls). It is used to control and manage engine-level operations. Important: The Control API has nothing to do with the General API (configured using the api section). The General API is an external-facing interface, exposed to interact with The CROWler to make data requests or post new sources. This section specifically configures the Control API, which operates within the CROWler Engine itself. Unlike the General API, which is designed for external interactions, the Control API is part of the CROWlerâ€™s Engine internal management system, and is not an external service.",
          "type": "object",
          "properties": {
            "host": {
              "title": "CROWler Engine Control API Host",
              "description": "This is the host that the CROWler will use to allow connections to the control API.",
              "type": "string",
              "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
            },
            "port": {
              "title": "CROWler Engine Control API Port",
              "type": "integer",
              "minimum": 1,
              "maximum": 65535,
              "description": "This is the port that the CROWler will use to allow connections to the control API.",
              "examples": [
                8081
              ]
            },
            "timeout": {
              "title": "CROWler Engine Control API Timeout",
              "type": "integer",
              "minimum": 10,
              "description": "This is the timeout (in seconds) for the control API. It is the maximum amount of time that the CROWler will wait for the control API to respond."
            },
            "sslmode": {
              "title": "CROWler Engine Control API SSL Mode",
              "type": "string",
              "description": "This is the sslmode switch for the control API. Use 'enable' to make the control API use HTTPS.",
              "enum": [
                "enable",
                "disable",
                ""
              ],
              "examples": [
                "enable",
                "disable"
              ]
            },
            "cert_file": {
              "title": "CROWler Engine Control API Certificate File",
              "type": "string",
              "description": "This is the certificate file for the Control API HTTPS protocol."
            },
            "key_file": {
              "title": "CROWler Engine Control API Key File",
              "type": "string",
              "description": "This is the full path to the key file for the Control API HTTPS certificates."
            },
            "rate_limit": {
              "title": "CROWler Engine Control API Requests Rate Limit",
              "type": "string",
              "description": "This is the rate limit for the control API. It is the maximum number of requests that the CROWler Engine will accept per second. You can use the ExprTerpreter language to set the rate limit. The format of this parameter is 'query_per_second, total_query' (for example: '100,100').",
              "examples": [
                "100,100",
                "125000,125000"
              ]
            },
            "readheader_timeout": {
              "title": "CROWler Engine Control API Readheader Timeout",
              "type": "integer",
              "minimum": 10,
              "description": "This is the readheader timeout (in seconds) for the Control API. It is the maximum amount of time that the CROWler will wait for the control API to respond.",
              "examples": [
                30
              ]
            },
            "write_timeout": {
              "title": "CROWler Engine Control API Write Timeout",
              "type": "integer",
              "minimum": 10,
              "description": "This is the write timeout (in seconds) for the Control API. It is the maximum amount of time that the CROWler will wait for the control API to respond."
            }
          },
          "additionalProperties": false
        }
      },
      "additionalProperties": false
    },
    "api": {
      "title": "CROWler General/Search API Configuration",
      "description": "This is the General/Search API configuration section, it's used to configure the General/Search API and it has no effect on the CROWler's engine, nor it has anything to do with the Engine's Control API. It is the configuration for the API that the CROWler will use to communicate with the outside world, to allow users to make queries, post Sources, check the status of the crawling activities etc.",
      "type": "object",
      "properties": {
        "host": {
          "title": "CROWler General/Search API Host",
          "description": "This is the host that the API will use to communicate with the outside world. Use 0.0.0.0 to make the API accessible from any IP address.",
          "type": "string",
          "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
        },
        "port": {
          "title": "CROWler General/Search API Port",
          "description": "This is the port that the API will use to communicate with the outside world.",
          "type": "integer",
          "minimum": 1,
          "maximum": 65535,
          "examples": [
            8080
          ]
        },
        "timeout": {
          "title": "CROWler General/Search API Timeout",
          "description": "This is the timeout for the API. It is the maximum amount of time that the CROWler will wait for the API to respond.",
          "type": "integer",
          "minimum": 10
        },
        "content_search": {
          "title": "CROWler General/Search API Content Search",
          "description": "This is a flag that tells the CROWler to search also in the text content field of a web object. This is useful for searching for every possible details of a web object, however will reduce performance quite a bit.",
          "type": "boolean"
        },
        "return_content": {
          "title": "CROWler General/Search API Return Content",
          "description": "This is a flag that tells the CROWler to return the web object's text content of a page in the search results. To improve performance, you can disable this option.",
          "type": "boolean"
        },
        "sslmode": {
          "title": "CROWler General/Search API SSL Mode",
          "description": "This is the sslmode switch for the API. Use 'enable' to make the API use HTTPS.",
          "type": "string",
          "enum": [
            "enable",
            "disable",
            ""
          ],
          "examples": [
            "enable",
            "disable"
          ]
        },
        "cert_file": {
          "title": "CROWler General/Search API Certificate File",
          "description": "This is the certificate file for the General/Search API HTTPS protocol.",
          "type": "string"
        },
        "key_file": {
          "title": "CROWler General/Search API Key File",
          "description": "This is the key file for the General/Search API HTTPS certificates.",
          "type": "string"
        },
        "rate_limit": {
          "title": "CROWler General/Search API Requests Rate Limit",
          "description": "This is the rate limit for the General/Search API. It is the maximum number of requests that the CROWler General API will accept per second. You can use the ExprTerpreter language to set the rate limit.",
          "type": "string"
        },
        "readheader_timeout": {
          "title": "CROWler General/Search API Readheader Timeout",
          "type": "integer",
          "minimum": 10,
          "description": "This is the readheader timeout (in seconds) for the General/Search API. It is the maximum amount of time that the CROWler will wait for the General/Search API to respond.",
          "examples": [
            30
          ]
        },
        "write_timeout": {
          "title": "CROWler Engine General/Search API Write Timeout",
          "type": "integer",
          "minimum": 10,
          "description": "This is the write timeout (in seconds) for the General/Search API. It is the maximum amount of time that the CROWler will wait for the control API to respond."
        },
        "enable_console": {
          "title": "CROWler General/Search API Enable Admin Console",
          "description": "This is a flag that tells the CROWler General API to enable the 'admin console' via the API. In other words, you'll get more endpoints to manage the CROWler via the General API instead of having to use local commands to do admin tasks.",
          "type": "boolean"
        },
        "return_404": {
          "title": "CROWler General/Search API Return 404",
          "description": "This is a flag that tells the CROWler to return 404 status code if a query has no results. This is mostly a secure measure to avoid leaking information about the CROWler's internal structure. If you are not exposing the General API to the public, you can disable this option.",
          "type": "boolean"
        }
      },
      "additionalProperties": false,
      "required": [
        "host",
        "timeout"
      ]
    },
    "selenium": {
      "title": "CROWler VDI access Configuration",
      "description": "This is the VDI configuration section, it's used to configure the VDI and tell the CROWler's Engine how to connect to it. It is the configuration for all the tools in the VDI image (for ex. selenium driver, Rbee etc.), to scale the CROWler web crawling capabilities, you can add multiple VDIs in an array format.",
      "type": "array",
      "items": {
        "title": "CROWler VDI Configuration Items",
        "type": "object",
        "properties": {
          "name": {
            "title": "CROWler VDI Name",
            "description": "This is the name of the VDI image. This is not a network name, so you can pick whatever makes sense for your business logic. This name can be used in a Source Configuration, to ensure the CROWler will use that specific VDI image to crawl the website.",
            "type": "string"
          },
          "location": {
            "title": "CROWler VDI Location",
            "description": "This is the location of the VDI image.",
            "type": "string"
          },
          "language": {
            "title": "CROWler VDI Language",
            "description": "This is the language oto set the VDI image to.",
            "type": "string",
            "examples": [
              "en",
              "de",
              "fr"
            ]
          },
          "path": {
            "title": "CROWler Selenium Path",
            "description": "This is the path to the selenium driver (IF LOCAL). It is the path to the selenium driver that the CROWler will use to crawl websites. (deprecated)",
            "type": "string"
          },
          "driver_path": {
            "title": "CROWler Selenium Driver Path",
            "description": "This is the path to the selenium driver (IF REMOTE). It is the path to the selenium driver that the CROWler will use to crawl websites. (deprecated)",
            "type": "string"
          },
          "use_service": {
            "title": "CROWler VDI Use Service (deprecated)",
            "description": "This is a flag that tells the CROWler to access Selenium as service. (deprecated)",
            "type": "boolean"
          },
          "type": {
            "title": "CROWler VDI Browser Type",
            "description": "This is the type of web browser the CROWler will use to crawl websites. For example, chrome or firefox (normally a VDI image has only one web Browser to reduce space).",
            "type": "string",
            "enum": [
              "chrome",
              "firefox",
              "chromium"
            ]
          },
          "port": {
            "title": "CROWler VDI Port",
            "description": "This is the VDI's API port.",
            "type": "integer",
            "minimum": 1,
            "maximum": 65535
          },
          "host": {
            "title": "CROWler VDI Host",
            "description": "This is the VDI host name or IP that the CROWler will use to connect to the VDI. It is the host that will be used to fetch web pages and that runs Selenium, RBee etc. For example, localhost. This is also the recommended way to use and connect to a VDI (in other words, don't try to run selenium, Rbee etc. locally, use a container for the VDI).",
            "type": "string",
            "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
          },
          "headless": {
            "title": "CROWler VDI Headless Mode",
            "description": "This is a flag that tells the VDI to run in headless mode. This is useful for running the selenium driver in a headless environment. It's generally NOT recommended to enable headless mode. (don't use headless unless you know what you're doing, headless browsing is mostly blocked these days!)",
            "type": "boolean"
          },
          "sslmode": {
            "title": "CROWler VDI SSL Mode",
            "description": "This is the sslmode. If set to `enable` then the CROWler Engine will try to connect to the VDI using HTTPS. Make sure the VDI is configured to accept HTTPS connections.",
            "type": "string",
            "enum": [
              "enable",
              "disable",
              ""
            ],
            "examples": [
              "enable",
              "disable"
            ]
          },
          "download_path": {
            "title": "CROWler VDI Downloaded files Path",
            "description": "This is the temporary download path for the VDI. It is the local path where the VDI will download files. This is useful for downloading files from websites (like pdf or zip etc.). The CROWler will use this path to temporarily store the downloaded files (before moving them to the storage files area).",
            "type": "string"
          },
          "proxy_url": {
            "title": "CROWler VDI Proxy Configuration",
            "description": "This is the proxy configuration for the VDI. It is the proxy that the VDI will use to connect to the internet. This is useful for bypassing firewalls or accessing websites that are blocked in your country.",
            "type": "string",
            "examples": [
              "http://proxy:port"
            ]
          },
          "sys_manager": {
            "title": "CROWler VDI System Manager",
            "description": "This configures the VDI System Manager API. It is the API that the CROWler will use to manage the VDI. This is used to configure system-wide proxy settings, manage the VDI's resources, and perform other system-level tasks.",
            "type": "object",
            "properties": {
              "port": {
                "title": "CROWler VDI System Manager Port",
                "description": "This is the port that the VDI System Manager will use to communicate with the outside world.",
                "type": "integer",
                "minimum": 1,
                "maximum": 65535
              },
              "timeout": {
                "title": "CROWler VDI System Manager Timeout",
                "description": "This is the timeout for the VDI System Manager. It is the maximum amount of time that the CROWler will wait for the VDI System Manager to respond.",
                "type": "integer",
                "minimum": 10
              },
              "sslmode": {
                "title": "CROWler VDI System Manager SSL Mode",
                "description": "This is the sslmode switch for the VDI System Manager. Use 'enable' to make the VDI System Manager use HTTPS.",
                "type": "string",
                "enum": [
                  "enable",
                  "disable",
                  ""
                ],
                "examples": [
                  "enable",
                  "disable"
                ]
              }
            }
          }
        },
        "additionalProperties": false,
        "required": [
          "type",
          "host"
        ]
      }
    },
    "prometheus": {
      "enabled": {
        "title": "CROWler Prometheus Exporter Enabled",
        "description": "This is a flag that tells the CROWler to enable the Prometheus Exporter. The Prometheus Exporter is a tool that allows you to monitor the CROWler's performance and health using Prometheus.",
        "type": "boolean"
      },
      "port": {
        "title": "CROWler Prometheus Exporter Port",
        "description": "This is the Prometheus Exporter Host Port that each CROWler engines will use to send their metrics.",
        "type": "integer",
        "minimum": 1,
        "maximum": 65535
      },
      "host": {
        "title": "CROWler Prometheus Exporter Host",
        "description": "This is the Prometheus Exporter host that each CROWler engines will use to send their metrics.",
        "type": "string",
        "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
      }
    },
    "events": {
      "title": "CROWler Events Manager Configuration (CEMI)",
      "description": "This is the Events Manager configuration section, it is used to configure the Events Manager",
      "type": "object",
      "properties": {
        "host": {
          "title": "CROWler Events Manager Host",
          "description": "This is the host that the Events Manager will use to communicate with the outside world.",
          "type": "string",
          "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
        },
        "port": {
          "title": "CROWler Events Manager Port",
          "description": "This is the port that the Events Manager will use to communicate with the outside world.",
          "type": "integer",
          "minimum": 1,
          "maximum": 65535
        },
        "timeout": {
          "title": "CROWler Events Manager Timeout",
          "description": "This is the timeout for the Events Manager. It is the maximum amount of time that the CROWler will wait for the Events Manager to respond.",
          "type": "integer",
          "minimum": 10
        },
        "sslmode": {
          "title": "CROWler Events Manager SSL Mode",
          "description": "This is the sslmode switch for the Events Manager. Use 'enable' to make the Events Manager use HTTPS.",
          "type": "string",
          "enum": [
            "enable",
            "disable",
            ""
          ],
          "examples": [
            "enable",
            "disable"
          ]
        },
        "cert_file": {
          "title": "CROWler Events Manager Certificate File",
          "description": "This is the certificate file for the Events Manager HTTPS protocol.",
          "type": "string"
        },
        "key_file": {
          "title": "CROWler Events Manager Key File",
          "description": "This is the key file for the Events Manager HTTPS certificates.",
          "type": "string"
        },
        "rate_limit": {
          "title": "CROWler Events Manager Requests Rate Limit",
          "description": "This is the rate limit for the Events Manager. It is the maximum number of requests that the CROWler Events Manager will accept per second. You can use the ExprTerpreter language to set the rate limit.",
          "type": "string"
        },
        "readheader_timeout": {
          "title": "CROWler Events Manager Readheader Timeout",
          "type": "integer",
          "minimum": 10,
          "description": "This is the readheader timeout (in seconds) for the Events Manager. It is the maximum amount of time that the CROWler will wait for the Events Manager to respond.",
          "examples": [
            30
          ]
        },
        "write_timeout": {
          "title": "CROWler Events Manager Write Timeout",
          "type": "integer",
          "minimum": 10,
          "description": "This is the write timeout (in seconds) for the Events Manager. It is the maximum amount of time that the CROWler will wait for the Events Manager to respond."
        },
        "automatic_events_removal": {
          "title": "CROWler Events Manager Automatic Events Removal",
          "description": "This field help automating events removal from the database. It supports one fo the following values: always, failed, success, never or empty string. If set to always it means remove the event always, if set to on_failure it means remove the event only if the handling fails, if set to on_success it means remove the event only if the handling is successful, if set to never it means never remove the event, if set to empty string it means the default behavior (always).",
          "enum": [
            "always",
            "on_failure",
            "on_success",
            "never",
            ""
          ]
        },
        "heartbeat_enabled": {
          "title": "CROWler Events Manager Heartbeat Enabled",
          "description": "This is a flag that tells the CROWler to enable the Events Manager heartbeat. The heartbeat is a periodic signal sent by the Events Manager to the entire fleet to indicate and collect that it is operational. This is useful for monitoring purposes.",
          "type": "boolean"
        },
        "heartbeat_interval": {
          "title": "CROWler Events Manager Heartbeat Interval",
          "description": "This is the interval for the Events Manager heartbeat. It is the amount of time between each heartbeat signal sent by the Events Manager to the entire fleet.",
          "type": "string",
          "examples": [
            "30s",
            "1m",
            "5m",
            "60 seconds"
          ]
        },
        "heartbeat_timeout": {
          "title": "CROWler Events Manager Heartbeat Timeout",
          "description": "This is the timeout for the Events Manager heartbeat. It is the maximum amount of time that the CROWler Events Manager will wait for the the fleet heartbeat responses before closing the response collection.",
          "type": "string",
          "examples": [
            "10s",
            "30s",
            "1m",
            "60 seconds"
          ]
        },
        "heartbeat_log": {
          "title": "CROWler Events Manager Heartbeat Log",
          "description": "This is a flag that tells the CROWler Events Manager to log the heartbeat responses from the fleet. This is useful for monitoring purposes.",
          "type": "boolean"
        },
        "sys_db_maintenance_schedule": {
          "title": "CROWler Events Manager DB Maintenance Interval scheduling",
          "description": "This option allows to schedule the Events Manager interval database operations. This operation maintains the DB in optima state and it's triggered only when the entire fleet is not processing sources or requests. The format of this parameters is string and can be read as 'every <X> minutes/hours/days'. For example, 'every 60 minutes' or 'every 1 hours' or 'every 1 days'.",
          "type": "string",
          "examples": [
            "60 minutes",
            "1 hour",
            "1 days"
          ]
        }
      },
      "additionalProperties": false
    },
    "image_storage": {
      "title": "CROWler Images Storage access Configuration",
      "description": "This is the Image Storage configuration section, it is used to tell the CROWler's Engine how and where to store collected images.",
      "type": "object",
      "properties": {
        "host": {
          "title": "CROWler Image Storage Host",
          "description": "This is the remote host for the image storage request.",
          "type": "string",
          "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
        },
        "path": {
          "title": "CROWler Image Storage Path",
          "description": "This is the path to the image storage. It is the path to the storage that the CROWler will use to store images. if the image storage is local, this is the path to the local directory where the images will be stored. If the image storage is remote, this is the path to the remote storage where the images will be stored.",
          "type": "string"
        },
        "port": {
          "title": "CROWler Image Storage Host Port",
          "description": "This is the remote port for the image storage request.",
          "type": "integer",
          "minimum": 1,
          "maximum": 65535
        },
        "region": {
          "title": "CROWler Image Storage Region",
          "description": "This is the region for the image storage request (for example for AWS s3 buckets).",
          "type": "string"
        },
        "token": {
          "title": "CROWler Image Storage Token",
          "description": "This is the token for the image storage request for remote storage.",
          "type": "string"
        },
        "secret": {
          "title": "CROWler Image Storage Secret",
          "description": "This is the secret for the image storage request for remote storage.",
          "type": "string"
        },
        "timeout": {
          "title": "CROWler Image Storage Timeout",
          "description": "This is the remote request timeout in seconds.",
          "type": "integer",
          "minimum": 10
        },
        "type": {
          "title": "CROWler Image Storage Type",
          "description": "This is the type of storage that the CROWler will use to store images. For example, s3, http or local (local is the default type).",
          "type": "string",
          "enum": [
            "s3",
            "http",
            "local",
            "ftp",
            ""
          ]
        },
        "sslmode": {
          "title": "CROWler Image Storage SSL Mode",
          "description": "This is the ssl mode for the image storage request for remote storage. Use enable to force https over http.",
          "type": "string",
          "enum": [
            "enable",
            "disable",
            ""
          ],
          "examples": [
            "enable",
            "disable"
          ]
        }
      },
      "additionalProperties": false,
      "required": [
        "path",
        "type"
      ]
    },
    "file_storage": {
      "title": "CROWler general Files Storage access Configuration",
      "description": "This is the File Storage configuration section, it is used to tell the CROWler's Engine how and where to store collected files.",
      "type": "object",
      "properties": {
        "host": {
          "title": "CROWler File Storage Host",
          "description": "This is the remote host for the file storage request.",
          "type": "string",
          "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
        },
        "path": {
          "title": "CROWler File Storage Path",
          "description": "This is the path to the file storage. It is the path to the storage that the CROWler will use to store files. if the file storage is local, this is the path to the local directory where the files will be stored. If the file storage is remote, this is the path to the remote storage where the files will be stored.",
          "type": "string"
        },
        "port": {
          "title": "CROWler File Storage Host Port",
          "description": "This is the remote port for the file storage request.",
          "type": "integer",
          "minimum": 1,
          "maximum": 65535
        },
        "region": {
          "title": "CROWler File Storage Region",
          "description": "This is the region for the file storage request (for example for AWS s3 buckets).",
          "type": "string"
        },
        "token": {
          "title": "CROWler File Storage Token",
          "description": "This is the token for the file storage request for remote storage.",
          "type": "string"
        },
        "secret": {
          "title": "CROWler File Storage Secret",
          "description": "This is the secret for the file storage request for remote storage.",
          "type": "string"
        },
        "timeout": {
          "title": "CROWler File Storage Timeout",
          "description": "This is the remote request timeout in seconds.",
          "type": "integer",
          "minimum": 10
        },
        "type": {
          "title": "CROWler File Storage Type",
          "description": "This is the type of storage that the CROWler will use to store files. For example, s3, http or local (local is the default type).",
          "type": "string",
          "enum": [
            "s3",
            "http",
            "local",
            "ftp",
            ""
          ]
        },
        "sslmode": {
          "title": "CROWler File Storage SSL Mode",
          "description": "This is the ssl mode for the file storage request for remote storage. Use enable to force https over http.",
          "type": "string",
          "enum": [
            "enable",
            "disable",
            ""
          ],
          "examples": [
            "enable",
            "disable"
          ]
        }
      },
      "additionalProperties": false,
      "required": [
        "path",
        "type"
      ]
    },
    "http_headers": {
      "title": "CROWler HTTP Headers collection Configuration",
      "description": "This is the HTTP Headers collection configuration section, it is used to tell the CROWler's Engine to collect HTTP headers and how when making requests to websites.",
      "type": "object",
      "properties": {
        "enabled": {
          "title": "CROWler HTTP Headers collection Enabled",
          "description": "This is a flag that tells the CROWler to collect HTTP headers. This is useful for detecting the headers of a website.",
          "type": "boolean"
        },
        "timeout": {
          "title": "CROWler HTTP Headers collection Timeout",
          "description": "This is the timeout for the HTTP headers collection. It is the maximum amount of time that the CROWler will wait for the HTTP headers to respond.",
          "type": "integer",
          "minimum": 5
        },
        "follow_redirects": {
          "title": "CROWler HTTP Headers collection Follow Redirects",
          "description": "This is a flag that tells the CROWler to follow redirects when collecting HTTP headers. This is useful for detecting the headers of a website.",
          "type": "boolean"
        },
        "ssl_discovery": {
          "title": "SSL discovery configuration",
          "type": "object",
          "additionalProperties": false,
          "properties": {
            "enabled": {
              "type": "boolean",
              "description": "Enable SSL discovery"
            },
            "jarm": {
              "type": "boolean",
              "description": "Collect JARM fingerprint"
            },
            "ja3": {
              "type": "boolean",
              "description": "Collect JA3 fingerprint (client hello)"
            },
            "ja3s": {
              "type": "boolean",
              "description": "Collect JA3S fingerprint (server hello)"
            },
            "ja4": {
              "type": "boolean",
              "description": "Collect JA4 fingerprint"
            },
            "ja4s": {
              "type": "boolean",
              "description": "Collect JA4S fingerprint"
            },
            "hassh": {
              "type": "boolean",
              "description": "Collect HASSH (client)"
            },
            "hassh_server": {
              "type": "boolean",
              "description": "Collect HASSH (server)"
            },
            "tlsh": {
              "type": "boolean",
              "description": "Compute TLSH hash"
            },
            "simhash": {
              "type": "boolean",
              "description": "Compute SimHash"
            },
            "minhash": {
              "type": "boolean",
              "description": "Compute MinHash"
            },
            "blake2": {
              "type": "boolean",
              "description": "Compute BLAKE2 hash"
            },
            "sha256": {
              "type": "boolean",
              "description": "Compute SHA-256 hash"
            },
            "cityhash": {
              "type": "boolean",
              "description": "Compute CityHash"
            },
            "murmurhash": {
              "type": "boolean",
              "description": "Compute MurmurHash"
            },
            "custom_tls": {
              "type": "boolean",
              "description": "Enable custom TLS probing"
            }
          },
          "required": [
            "enabled"
          ]
        },
        "proxies": {
          "title": "CROWler HTTP Headers collection Proxies",
          "description": "This is the list of proxies that the CROWler will use to collect HTTP headers.",
          "type": "array",
          "items": {
            "title": "CROWler HTTP Headers collection Proxies Items",
            "type": "object",
            "properties": {
              "host": {
                "title": "CROWler HTTP Headers collection Proxy Host",
                "description": "This is the proxy host that the CROWler will use to collect HTTP headers. It is the proxy host that the CROWler will use to collect HTTP headers.",
                "type": "string",
                "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
              },
              "port": {
                "title": "CROWler HTTP Headers collection Proxy Port",
                "description": "This is the proxy port that the CROWler will use to collect HTTP headers. It is the proxy port that the CROWler will use to collect HTTP headers.",
                "type": "integer",
                "minimum": 1,
                "maximum": 65535
              },
              "username": {
                "title": "CROWler HTTP Headers collection Proxy Username",
                "description": "This is the proxy username that the CROWler will use to collect HTTP headers. It is the proxy username that the CROWler will use to collect HTTP headers.",
                "type": "string"
              },
              "password": {
                "title": "CROWler HTTP Headers collection Proxy Password",
                "description": "This is the proxy password that the CROWler will use to collect HTTP headers. It is the proxy password that the CROWler will use to collect HTTP headers.",
                "type": "string"
              }
            },
            "additionalItems": false
          }
        }
      },
      "additionalProperties": false
    },
    "network_info": {
      "title": "CROWler Network Information collection Configuration",
      "description": "This is the network information collection configuration section, it is used to tell the CROWler's Engine how and what network information we want to collect for each discovered entity within the crawling of a Source. This section is also used to tell the CROWler if we want to detect network vulnerabilities, open ports, etc. Collect Whois information, DNS information, and more.",
      "type": "object",
      "properties": {
        "dns": {
          "title": "CROWler Network Information collection DNS Configuration",
          "description": "This is the configuration for the DNS data collection. It is the configuration for the DNS data collection that the CROWler will use to detect the IP address of a domain, subdomains etc.",
          "type": "object",
          "properties": {
            "enabled": {
              "title": "CROWler Network Information collection DNS Enabled",
              "description": "This is a flag that tells the CROWler to use DNS techniques. This is useful for detecting the IP address of a domain.",
              "type": "boolean"
            },
            "timeout": {
              "title": "CROWler Network Information collection DNS Timeout",
              "description": "This is the timeout for the DNS database. It is the maximum amount of time that the CROWler will wait for the DNS database to respond.",
              "type": "integer",
              "minimum": 5
            },
            "rate_limit": {
              "title": "CROWler Network Information collection DNS Rate Limit",
              "description": "This is the rate limit for the DNS database. It is the maximum number of requests that the CROWler will send to the DNS database per second. You can use the ExprTerpreter language to set the rate limit.",
              "type": "string"
            }
          },
          "additionalProperties": false,
          "required": [
            "enabled"
          ]
        },
        "whois": {
          "title": "CROWler Network Information collection Whois Configuration",
          "description": "This is the configuration for the whois data collection. It is the configuration for the whois data collection that the CROWler will use to detect the owner of a domain.",
          "type": "object",
          "properties": {
            "enabled": {
              "title": "CROWler Network Information collection Whois Enabled",
              "description": "This is a flag that tells the CROWler to use whois techniques. This is useful for detecting the owner of a domain.",
              "type": "boolean"
            },
            "timeout": {
              "title": "CROWler Network Information collection Whois Timeout",
              "description": "This is the timeout for the whois database. It is the maximum amount of time that the CROWler will wait for the whois database to respond.",
              "type": "integer",
              "minimum": 5
            },
            "rate_limit": {
              "title": "CROWler Network Information collection Whois Rate Limit",
              "description": "This is the rate limit for the whois database. It is the maximum number of requests that the CROWler will send to the whois database per second. You can use the ExprTerpreter language to set the rate limit.",
              "type": "string"
            }
          },
          "additionalProperties": false,
          "required": [
            "enabled"
          ]
        },
        "netlookup": {
          "title": "CROWler Network Information collection Netlookup Configuration",
          "description": "This is the configuration for the netlookup data collection. It is the configuration for the netlookup data collection that the CROWler will use to detect the network information of a host.",
          "type": "object",
          "properties": {
            "enabled": {
              "title": "CROWler Network Information collection Netlookup Enabled",
              "description": "This is a flag that tells the CROWler to use netlookup techniques. This is useful for detecting the network information of a host.",
              "type": "boolean"
            },
            "timeout": {
              "title": "CROWler Network Information collection Netlookup Timeout",
              "description": "This is the timeout for the netlookup database. It is the maximum amount of time that the CROWler will wait for the netlookup database to respond.",
              "type": "integer",
              "minimum": 5
            },
            "rate_limit": {
              "title": "CROWler Network Information collection Netlookup Rate Limit",
              "description": "This is the rate limit for the netlookup database. It is the maximum number of requests that the CROWler will send to the netlookup database per second. You can use the ExprTerpreter language to set the rate limit.",
              "type": "string"
            }
          },
          "additionalProperties": false,
          "required": [
            "enabled"
          ]
        },
        "geo_localization": {
          "title": "CROWler Network Information collection Geolocation Configuration",
          "description": "This is the configuration for the geolocation data collection. It is the configuration for the geolocation data collection that the CROWler will use to detect the location of a host.",
          "type": "object",
          "properties": {
            "enabled": {
              "title": "CROWler Network Information collection Geolocation Enabled",
              "description": "This is a flag that tells the CROWler to use geolocation techniques. This is useful for detecting the location of a host.",
              "type": "boolean"
            },
            "path": {
              "title": "CROWler Network Information collection Geolocation Path",
              "description": "This is the path to the geolocation database. It is the path to the database that the CROWler will use to determine the location of a host.",
              "type": "string"
            },
            "type": {
              "title": "CROWler Network Information collection Geolocation Type",
              "description": "This is the type of geolocation database that the CROWler will use. It is the type of database that the CROWler will use to determine the location of a host. For example maxmind or ip2location",
              "type": "string",
              "enum": [
                "maxmind",
                "ip2location",
                ""
              ]
            },
            "timeout": {
              "title": "CROWler Network Information collection Geolocation Timeout",
              "description": "This is the timeout for the geolocation database. It is the maximum amount of time that the CROWler will wait for the geolocation database to respond.",
              "type": "integer",
              "minimum": 5
            },
            "api_key": {
              "title": "CROWler Network Information collection Geolocation API Key",
              "description": "This is the API key for the geolocation database. It is the API key that the CROWler will use to connect to the geolocation database.",
              "type": "string"
            },
            "sslmode": {
              "title": "CROWler Network Information collection Geolocation SSL Mode",
              "description": "This is the sslmode that the CROWler will use to connect to the geolocation database.",
              "type": "string",
              "enum": [
                "enable",
                "disable",
                ""
              ],
              "examples": [
                "enable",
                "disable"
              ]
            }
          },
          "additionalProperties": false,
          "required": [
            "enabled",
            "path"
          ]
        },
        "service_scout": {
          "title": "Service Scout Configuration",
          "description": "This is the configuration for the service scout data collection. It is the configuration for the service scout data collection that the CROWler will use to detect services that are running on a host, network vulnerabilities, network software versions etc.",
          "type": "object",
          "properties": {
            "enabled": {
              "title": "Service Enabled",
              "description": "This is a flag that tells the CROWler to use service scanning techniques. This is useful for detecting services that are running on a host.",
              "type": "boolean"
            },
            "timeout": {
              "title": "Service Timeout",
              "description": "This is the timeout for the scan. It is the maximum amount of time that the CROWler will wait for a host to respond to a scan.",
              "type": "integer",
              "minimum": 5
            },
            "idle_scan": {
              "title": "Service Idle Scan Config",
              "description": "This is the configuration for the idle scan.",
              "type": "object",
              "properties": {
                "host": {
                  "title": "Idle Scan Host",
                  "description": "Host FQDN or IP address.",
                  "type": "string",
                  "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
                },
                "port": {
                  "title": "Idle Scan Host Port",
                  "type": "integer",
                  "minimum": 1,
                  "maximum": 65535,
                  "description": "Port number."
                }
              },
              "additionalProperties": false
            },
            "ping_scan": {
              "title": "Ping Scan",
              "description": "This is a flag that tells the CROWler to use ping scanning techniques. This is useful for detecting hosts that are alive.",
              "type": "boolean"
            },
            "connect_scan": {
              "title": "Connect Scan",
              "description": "This is a flag that tells the CROWler to use connect scanning techniques. This is useful for detecting services that are running on a host.",
              "type": "boolean"
            },
            "syn_scan": {
              "title": "SYN Scan",
              "description": "This is a flag that tells the CROWler to use SYN scanning techniques. This is useful for detecting services that are running on a host.",
              "type": "boolean"
            },
            "udp_scan": {
              "title": "UDP Scan",
              "description": "This is a flag that tells the CROWler to use UDP scanning techniques. This is useful for detecting services that are running on a host.",
              "type": "boolean"
            },
            "no_dns_resolution": {
              "title": "No DNS Resolution",
              "description": "This is a flag that tells the CROWler not to resolve hostnames to IP addresses. This is useful for avoiding detection by intrusion detection systems.",
              "type": "boolean"
            },
            "service_detection": {
              "title": "Service Detection",
              "description": "This is a flag that tells the CROWler to use service detection techniques. This is useful for detecting services that are running on a host.",
              "type": "boolean"
            },
            "service_db": {
              "title": "Service DB file",
              "description": "This is the service detection database.",
              "type": "string"
            },
            "os_finger_print": {
              "title": "OS Fingerprinting",
              "description": "This is a flag that tells the CROWler to use OS fingerprinting techniques. This is useful for detecting the operating system that is running on a host.",
              "type": "boolean"
            },
            "aggressive_scan": {
              "title": "Aggressive Scan",
              "description": "This is a flag that tells the CROWler to use aggressive scanning techniques. This is useful for detecting services that are running on a host.",
              "type": "boolean"
            },
            "script_scan": {
              "title": "Script Scan",
              "description": "This is a list of nmap and crowler-scanner scripts to run. This is particularly important when a user wants to do vulnerability scanning.",
              "type": "array",
              "items": {
                "type": "string",
                "examples": [
                  "http-enum",
                  "http-headers",
                  "default",
                  "vuln"
                ]
              }
            },
            "excluded_hosts": {
              "title": "Excluded Hosts",
              "description": "This is a list of hosts to exclude from the scan. The CROWler may encounter such hosts during its crawling activities, so this field makes it easy to define a list of hosts that it should always avoid scanning.",
              "type": "array",
              "items": {
                "type": "string",
                "oneOf": [
                  {
                    "format": "ipv4"
                  },
                  {
                    "format": "ipv6"
                  },
                  {
                    "format": "hostname"
                  }
                ],
                "examples": [
                  "example.com",
                  "192.168.0.1",
                  "localhost",
                  "2001:0db8:85a3:0000:0000:8a2e:0370:7334"
                ]
              },
              "additionalProperties": false
            },
            "timing_template": {
              "title": "Timing Template",
              "description": "This allows the user to set the timing template for the scan. The timing template is a string that is passed to nmap to set the timing of the scan. DO not specify values using Tx, where x is a number. Instead, use just the number, e.g., '3'.",
              "type": "string",
              "examples": [
                "3"
              ]
            },
            "host_timeout": {
              "title": "Host Timeout",
              "description": "This is the timeout for the scan. It is the maximum amount of time that the CROWler will wait for a host to respond to a scan.",
              "type": "string"
            },
            "min_rate": {
              "title": "Minimum Rate",
              "description": "This is the minimum rate at which the CROWler will scan hosts. It is the minimum number of packets that the CROWler will send to a host per second.",
              "type": "string"
            },
            "max_retries": {
              "title": "Maximum Retries",
              "description": "This is the maximum number of times that the CROWler will retry a scan on a host. If the CROWler is unable to scan a host after this number of retries, it will move on to the next host.",
              "type": "integer"
            },
            "source_port": {
              "title": "Source Port",
              "description": "This is the source port that the CROWler will use for scanning. It is the port that the CROWler will use to send packets to hosts.",
              "type": "integer",
              "minimum": 1,
              "maximum": 65535,
              "examples": [
                80
              ]
            },
            "interface": {
              "title": "Interface",
              "description": "This is the interface that the CROWler will use for scanning. It is the network interface that the CROWler will use to send packets to hosts. Use this option with a port that is behind a VPN or a proxy for better results.",
              "type": "string",
              "examples": [
                "eth0"
              ]
            },
            "spoof_ip": {
              "title": "Spoof IP",
              "description": "This is the IP address that the CROWler will use to spoof its identity. It is the IP address that the CROWler will use to send packets to hosts. Use this option with a port that is behind a VPN or a proxy for better results.",
              "type": "string",
              "examples": [
                "192.168.0.1"
              ]
            },
            "randomize_hosts": {
              "title": "Randomize Hosts List",
              "description": "This is a flag that tells the CROWler to randomize the order in which it scans hosts. This is useful for avoiding detection by intrusion detection systems.",
              "type": "boolean"
            },
            "data_length": {
              "title": "Data Length",
              "description": "This is the length of the data that the CROWler will send to hosts. It is the length of the data that the CROWler will use to send packets to hosts. Use this option with a port that is behind a VPN or a proxy for better results.",
              "type": "integer"
            },
            "delay": {
              "title": "Delay",
              "description": "This is the delay between packets that the CROWler will use for scanning. It is the delay between packets that the CROWler will use to send packets to hosts. Use this option with a port that is behind a VPN or a proxy for better results. For the delay you can also use the CROWler exprterpreter to generate delay values at runtime, e.g., 'random(1, 3)' or 'random(random(1,3), random(5,8))'.",
              "type": "string"
            },
            "mtu_discovery": {
              "title": "MTU Discovery",
              "description": "This is a flag that tells the CROWler to use MTU discovery when scanning hosts. This is useful for avoiding detection by intrusion detection systems.",
              "type": "boolean"
            },
            "scan_flags": {
              "title": "Scan Flags",
              "description": "This is the flags that the CROWler will use for scanning. It is the flags that the CROWler will use to send packets to hosts. Use this option with a port that is behind a VPN or a proxy for better results.",
              "type": "string"
            },
            "ip_fragment": {
              "title": "IP Fragment",
              "description": "This is a flag that tells the CROWler to fragment IP packets. This is useful for avoiding detection by intrusion detection systems.",
              "type": "boolean"
            },
            "min_port_number": {
              "title": "Minimum Port Number",
              "description": "This is the minimum port number to scan (default is 1).",
              "type": "integer",
              "minimum": 1
            },
            "max_port_number": {
              "title": "Maximum Port Number",
              "description": "This is the maximum port number to scan (default is 9000).",
              "type": "integer",
              "maximum": 65535
            },
            "max_parallelism": {
              "title": "Maximum Parallelism",
              "description": "This is the maximum number of parallelism used to provide scans for a single target. Multiple targets are ALWAYS scanned in parallel.",
              "type": "integer"
            },
            "dns_servers": {
              "title": "DNS Servers",
              "description": "This is a list of custom DNS servers.",
              "type": "array",
              "items": {
                "type": "string",
                "examples": [
                  "1.1.1.1"
                ]
              },
              "additionalProperties": false
            },
            "proxies": {
              "title": "Proxies",
              "description": "List of Proxies to use to perform a scan.",
              "type": "array",
              "items": {
                "type": "string"
              },
              "additionalProperties": false
            }
          },
          "additionalProperties": false,
          "required": [
            "enabled"
          ]
        }
      },
      "additionalProperties": false
    },
    "rulesets_schema_path": {
      "title": "CROWler Ruleset JSON Schema location Configuration",
      "description": "This is the path to the rulesets schema. It is the path to the schema that the CROWler will use to validate the rulesets.",
      "type": "string",
      "examples": [
        "./schemas/rulesets.schema.json"
      ]
    },
    "rulesets": {
      "title": "CROWler Rulesets locations Configuration",
      "description": "This is the rulesets load configuration section, it is used to tell the CROWler where and how to load all the Rulesets we want to use to crawl, interact, scrape info and detect stuff on the provided Sources to crawl.",
      "type": "array",
      "items": {
        "title": "CROWler Rulesets locations Configuration parameters",
        "type": "object",
        "properties": {
          "path": {
            "title": "CROWler Rulesets Path",
            "description": "This is the path that the CROWler will use to fetch the ruleset. You can use wildcard to fetch multiple rulesets. for example './rules/*.yaml'.",
            "type": "array",
            "items": {
              "type": "string"
            },
            "examples": [
              [
                "./rules/*.yaml"
              ]
            ]
          },
          "host": {
            "title": "CROWler Rulesets Host",
            "description": "This is the host that the CROWler will use to fetch the ruleset.",
            "type": "string",
            "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
          },
          "port": {
            "title": "CROWler Rulesets Host Port",
            "description": "This is the port that the CROWler will use to fetch the ruleset.",
            "type": "string",
            "examples": [
              "8082"
            ]
          },
          "region": {
            "title": "CROWler Rulesets Host Region",
            "description": "This is the region that the CROWler will use to fetch the ruleset (for example, if you're hosting your ruleset on an AWS S3 bucket).",
            "type": "string"
          },
          "token": {
            "title": "CROWler Rulesets Host access Token",
            "description": "This is the token that the CROWler will use to connect to the distribution server to download the ruleset.",
            "type": "string"
          },
          "secret": {
            "title": "CROWler Rulesets Host access Secret",
            "description": "This is the secret that the CROWler will use to connect to the distribution server to download the ruleset.",
            "type": "string"
          },
          "timeout": {
            "title": "CROWler Rulesets Host Timeout",
            "description": "This is the timeout in seconds for the CROWler to fetch the ruleset.",
            "type": "integer",
            "minimum": 10
          },
          "type": {
            "title": "CROWler Rulesets Host Type",
            "description": "This is the type of the distribution server that the CROWler will use to fetch the ruleset. For example, s3, http or local. (local is default)",
            "type": "string",
            "enum": [
              "s3",
              "http",
              "local",
              "ftp",
              ""
            ]
          },
          "sslmode": {
            "title": "CROWler Rulesets Host SSL Mode",
            "description": "This is the sslmode that the CROWler will use to connect to the distribution server to fetch the ruleset. Use 'enable' to force https over http.",
            "type": "string",
            "enum": [
              "enable",
              "disable",
              ""
            ],
            "examples": [
              "enable",
              "disable"
            ]
          },
          "refresh": {
            "title": "CROWler Rulesets Automatic Refresh Interval",
            "description": "This is the refresh interval in seconds for the CROWler to fetch the ruleset (refresh it).",
            "type": "integer",
            "minimum": 0
          }
        },
        "additionalProperties": false,
        "required": [
          "path",
          "type"
        ]
      },
      "minItems": 1,
      "uniqueItems": true
    },
    "agents": {
      "title": "CROWler Agents Configuration and locations",
      "description": "This is the CROWler Agents configuration section. The Agents are used to process Data, Events and more. They are used to extend the CROWler's capabilities, without coding. Agents can be AI based or simple sets of actions, plugins and/or both. Each Agent is defined using YAML or JSON files.",
      "type": "array",
      "items": {
        "title": "CROWler Agents Configuration parameters",
        "type": "object",
        "properties": {
          "global_parameters": {
            "title": "CROWler Agents Global Parameters",
            "description": "This is the global parameters that the CROWler will pass to each agent in this batch load.",
            "type": "object",
            "properties": {},
            "additionalProperties": true
          },
          "path": {
            "title": "CROWler Agents load paths",
            "description": "This is the path that the CROWler will use to fetch the yaml or json file that defines one or more agents. You can use wildcard to fetch multiple files. for example './agents/*.yaml'.",
            "type": "array",
            "items": {
              "type": "string"
            },
            "examples": [
              [
                "./agents/*.yaml"
              ]
            ]
          },
          "host": {
            "title": "CROWler Agents Host",
            "description": "This is the host that the CROWler will use to fetch the agents.",
            "type": "string",
            "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
          },
          "port": {
            "title": "CROWler Agents Host Port",
            "description": "This is the port that the CROWler will use to fetch the agents.",
            "type": "string",
            "examples": [
              "8082"
            ]
          },
          "region": {
            "title": "CROWler Agents Host Region",
            "description": "This is the region that the CROWler will use to fetch the agents (for example, if you're hosting your agents on an AWS S3 bucket).",
            "type": "string"
          },
          "token": {
            "title": "CROWler Agents Host access Token",
            "description": "This is the token that the CROWler will use to connect to the distribution server to download the agents.",
            "type": "string"
          },
          "secret": {
            "title": "CROWler Agents Host access Secret",
            "description": "This is the secret that the CROWler will use to connect to the distribution server to download the agents.",
            "type": "string"
          },
          "timeout": {
            "title": "CROWler Agents Host Timeout",
            "description": "This is the timeout in seconds for the CROWler to fetch the agents.",
            "type": "integer",
            "minimum": 10
          },
          "agents_timeout": {
            "title": "CROWler Agents Timeout",
            "description": "This is the timeout in seconds for the CROWler to execute an agent.",
            "type": "integer",
            "minimum": 30
          },
          "plugins_timeout": {
            "title": "CROWler Agents Plugins Timeout",
            "description": "This is the timeout in seconds for the CROWler to execute a plugin.",
            "type": "integer",
            "minimum": 30
          },
          "type": {
            "title": "CROWler Agents Host Type",
            "description": "This is the type of the distribution server that the CROWler will use to fetch the agents. For example, s3, http or local. (local is default)",
            "type": "string",
            "enum": [
              "s3",
              "http",
              "local",
              "ftp",
              ""
            ]
          },
          "sslmode": {
            "title": "CROWler Agents Host SSL Mode",
            "description": "This is the sslmode that the CROWler will use to connect to the distribution server to fetch the agents. Use 'enable' to force https over http.",
            "type": "string",
            "enum": [
              "enable",
              "disable",
              ""
            ],
            "examples": [
              "enable",
              "disable"
            ]
          },
          "refresh": {
            "title": "CROWler Agents Automatic Refresh Interval",
            "description": "This is the refresh interval in seconds for the CROWler to fetch the agents (refresh them).",
            "type": "integer",
            "minimum": 0
          }
        },
        "additionalProperties": false,
        "required": [
          "path",
          "type"
        ]
      },
      "minItems": 1,
      "uniqueItems": true
    },
    "plugins": {
      "title": "CROWler Plugins locations Configuration",
      "description": "This is the Plugins configuration section, it is used to tell the CROWler's Engine which plugins we want to use and provide the path to the plugins. Plugins are used to extend the CROWler's capabilities, for example, to detect phishing URLs, detect malware, detect network vulnerabilities, etc.",
      "type": "object",
      "properties": {
        "global_parameters": {
          "title": "CROWler Plugins Global Parameters",
          "description": "This is the global parameters that the CROWler will pass to each plugin in this batch load.",
          "type": "object",
          "properties": {},
          "additionalProperties": true
        },
        "timeout": {
          "title": "CROWler Plugin Execution Timeout",
          "description": "This is the timeout in seconds for the CROWler to execute a plugin.",
          "type": "integer",
          "minimum": 10
        },
        "plugins_timeout": {
          "title": "CROWler Plugins Execution Timeout",
          "description": "This is the timeout in seconds for the CROWler to execute a plugin.",
          "type": "integer",
          "minimum": 15
        },
        "locations": {
          "title": "CROWler Plugins Locations List",
          "description": "This is the list of locations from where the CROWler will load and register all available plugins.",
          "type": "array",
          "items": {
            "title": "CROWler Plugins Location Configuration",
            "type": "object",
            "properties": {
              "path": {
                "title": "CROWler Plugins location Path",
                "description": "This is the path to the plugin.",
                "type": "array",
                "items": {
                  "type": "string",
                  "description": "This is a list of paths from where the CROWler will search for plugins to load.",
                  "examples": [
                    "./plugins/"
                  ]
                }
              },
              "host": {
                "title": "CROWler Plugins location Host",
                "description": "This is the host that the CROWler will use to fetch the plugin.",
                "type": "string",
                "pattern": "^(((([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])(\\.([a-zA-Z0-9\\-]+))*)|(\\[([0-9a-fA-F]{1,4}\\:{1,2}){7}[0-9a-fA-F]{1,4}\\])|(\\${[A-Za-z_][A-Za-z0-9_]*}))$"
              },
              "port": {
                "title": "CROWler Plugins location Host Port",
                "description": "This is the port that the CROWler will use to fetch the plugin.",
                "type": "string",
                "examples": [
                  "8082"
                ]
              },
              "region": {
                "title": "CROWler Plugins location Host Region",
                "description": "This is the region that the CROWler will use to fetch the plugin (for example, if you're hosting your plugin on an AWS S3 bucket).",
                "type": "string"
              },
              "token": {
                "title": "CROWler Plugins location Host access Token",
                "description": "This is the token that the CROWler will use to connect to the distribution server to download the plugin.",
                "type": "string"
              },
              "secret": {
                "title": "CROWler Plugins location Host access Secret",
                "description": "This is the secret that the CROWler will use to connect to the distribution server to download the plugin.",
                "type": "string"
              },
              "timeout": {
                "title": "CROWler Plugins location Host Timeout",
                "description": "This is the timeout in seconds for the CROWler to fetch the plugin.",
                "type": "integer",
                "minimum": 10
              },
              "type": {
                "title": "CROWler Plugins location Host Type",
                "description": "This is the type of the distribution server that the CROWler will use to fetch the plugin. For example, s3, http or local. (local is default)",
                "type": "string",
                "enum": [
                  "s3",
                  "http",
                  "local",
                  "ftp",
                  ""
                ]
              },
              "sslmode": {
                "title": "CROWler Plugins location Host SSL Mode",
                "description": "This is the sslmode that the CROWler will use to connect to the distribution server to fetch the plugin. Use 'enable' to force https over http.",
                "type": "string",
                "enum": [
                  "enable",
                  "disable",
                  ""
                ],
                "examples": [
                  "enable",
                  "disable"
                ]
              },
              "refresh": {
                "title": "CROWler Plugins location Automatic Refresh Interval",
                "description": "This is the refresh interval in seconds for the CROWler to fetch the plugin (refresh it). Zero means no refresh.",
                "type": "integer",
                "minimum": 0
              }
            },
            "additionalProperties": false
          }
        }
      },
      "additionalProperties": false
    },
    "external_detection": {
      "title": "CROWler External Detection Services Configuration",
      "description": "This is the External Detection configuration section, it is used to tell the CROWler's Engine which external detection services we want to use and provide credentials to access them. External detection services are provided by VirusTotal, URLHaus, PhishTank, GoogleSafeBrowsing, AbuseIPDB, OpenPhish, Cuckoo, HybridAnalysis, CiscoUmbrella, AlienVault, IPVoid, Shodan, Censys, SSLLabs. They can be accessed using the CROWler's Detection Rules.",
      "type": "object",
      "properties": {
        "abuse_ipdb": {
          "title": "AbuseIPDB Configuration",
          "description": "This is the AbuseIPDB configuration section, it is used to tell the CROWler's Engine how to connect to AbuseIPDB and use its services. AbuseIPDB will be accessed through CROWler's Detection Rules.",
          "type": "object",
          "properties": {
            "api_key": {
              "title": "AbuseIPDB API Key",
              "description": "This is the API key that the CROWler will use to connect to AbuseIPDB.",
              "type": "string"
            }
          },
          "additionalProperties": false,
          "required": [
            "api_key"
          ]
        },
        "alien_vault": {
          "title": "Alien Vault Configuration",
          "description": "This is the Alien Vault configuration section, it is used to tell the CROWler's Engine how to connect to Alien Vault and use its services. Alien Vault will be accessed through CROWler's Detection Rules.",
          "type": "object",
          "properties": {
            "api_key": {
              "title": "Alien Vault API Key",
              "description": "This is the API key that the CROWler will use to connect to Alien Vault.",
              "type": "string"
            }
          },
          "additionalProperties": false,
          "required": [
            "api_key"
          ]
        },
        "censys": {
          "title": "Censys Configuration",
          "description": "This is the Censys configuration section, it is used to tell the CROWler's Engine how to connect to Censys and use its services. Censys will be accessed through CROWler's Detection Rules.",
          "type": "object",
          "properties": {
            "api_id": {
              "title": "Censys API ID",
              "description": "This is the API ID that the CROWler will use to connect to Censys.",
              "type": "string"
            },
            "api_secret": {
              "title": "Censys API Secret",
              "description": "This is the API secret that the CROWler will use to connect to Censys.",
              "type": "string"
            }
          },
          "additionalProperties": false,
          "required": [
            "api_id",
            "api_secret"
          ]
        },
        "cisco_umbrella": {
          "title": "Cisco Umbrella Configuration",
          "description": "This is the Cisco Umbrella configuration section, it is used to tell the CROWler's Engine how to connect to Cisco Umbrella and use its services. Cisco Umbrella will be accessed through CROWler's Detection Rules.",
          "type": "object",
          "properties": {
            "api_key": {
              "title": "Cisco Umbrella API Key",
              "description": "This is the API key that the CROWler will use to connect to Cisco Umbrella.",
              "type": "string"
            }
          },
          "additionalProperties": false,
          "required": [
            "api_key"
          ]
        },
        "cuckoo": {
          "title": "Cuckoo Configuration",
          "description": "This is the Cuckoo configuration section, it is used to tell the CROWler's Engine how to connect to Cuckoo and use its services. Cuckoo will be accessed through CROWler's Detection Rules.",
          "type": "object",
          "properties": {},
          "additionalProperties": false
        },
        "google_safe_browsing": {
          "title": "Google Safe Browsing Configuration",
          "description": "This is the Google Safe Browsing configuration section, it is used to tell the CROWler's Engine how to connect to Google Safe Browsing and use its services. Google Safe Browsing will be accessed through CROWler's Detection Rules.",
          "type": "object",
          "properties": {
            "company_id": {
              "title": "Google Safe Browsing Company ID",
              "description": "This is the company ID that the CROWler will use to connect to Google Safe Browsing.",
              "type": "string"
            },
            "api_key": {
              "title": "Google Safe Browsing API Key",
              "description": "This is the API key that the CROWler will use to connect to Google Safe Browsing.",
              "type": "string"
            }
          },
          "additionalProperties": false,
          "required": [
            "api_key"
          ]
        },
        "hybrid_analysis": {
          "title": "Hybrid Analysis Configuration",
          "description": "This is the Hybrid Analysis configuration section, it is used to tell the CROWler's Engine how to connect to Hybrid Analysis and use its services. Hybrid Analysis will be accessed through CROWler's Detection Rules.",
          "type": "object",
          "properties": {
            "api_key": {
              "title": "Hybrid Analysis API Key",
              "description": "This is the API key that the CROWler will use to connect to Hybrid Analysis.",
              "type": "string"
            }
          },
          "additionalProperties": false,
          "required": [
            "api_key"
          ]
        },
        "ipvoid": {
          "title": "IPVoid Configuration",
          "description": "This is the IPVoid configuration section, it is used to tell the CROWler's Engine how to connect to IPVoid and use its services. IPVoid will be accessed through CROWler's Detection Rules.",
          "type": "object",
          "properties": {
            "api_key": {
              "title": "IPVoid API Key",
              "description": "This is the API key that the CROWler will use to connect to IPVoid.",
              "type": "string"
            }
          },
          "additionalProperties": false,
          "required": [
            "api_key"
          ]
        },
        "open_phish": {
          "title": "OpenPhish Configuration",
          "description": "This is the OpenPhish configuration section, it is used to tell the CROWler's Engine how to connect to OpenPhish and use its services. OpenPhish will be accessed through CROWler's Detection Rules.",
          "type": "object",
          "properties": {
            "api_key": {
              "title": "OpenPhish API Key",
              "description": "This is the API key that the CROWler will use to connect to OpenPhish.",
              "type": "string"
            }
          },
          "additionalProperties": false,
          "required": [
            "api_key"
          ]
        },
        "phish_tank": {
          "title": "PhishTank Configuration",
          "description": "This is the PhishTank configuration section, it is used to tell the CROWler's Engine how to connect to PhishTank and use its services. PhishTank will be accessed through CROWler's Detection Rules.",
          "type": "object",
          "properties": {},
          "additionalProperties": false
        },
        "shodan": {
          "title": "Shodan Configuration",
          "description": "This is the Shodan configuration section, it is used to tell the CROWler's Engine how to connect to Shodan and use its services. Shodan will be accessed through CROWler's Detection Rules.",
          "type": "object",
          "properties": {
            "api_key": {
              "title": "Shodan API Key",
              "description": "This is the API key that the CROWler will use to connect to Shodan.",
              "type": "string"
            }
          },
          "additionalProperties": false,
          "required": [
            "api_key"
          ]
        },
        "ssllabs": {
          "title": "SSLLabs Configuration",
          "description": "This is the SSLLabs configuration section, it is used to tell the CROWler's Engine how to connect to SSLLabs and use its services. SSLLabs will be accessed through CROWler's Detection Rules.",
          "type": "object",
          "properties": {
            "api_key": {
              "title": "SSLLabs API Key",
              "description": "This is the API key that the CROWler will use to connect to SSLLabs.",
              "type": "string"
            }
          },
          "additionalProperties": false
        },
        "url_haus": {
          "title": "URLHaus Configuration",
          "description": "This is the URLHaus configuration section, it is used to tell the CROWler's Engine how to connect to URLHaus and use its services. URLHaus will be accessed through CROWler's Detection Rules.",
          "type": "object",
          "properties": {},
          "additionalProperties": false
        },
        "virus_total": {
          "title": "VirusTotal Configuration",
          "description": "This is the VirusTotal configuration section, it is used to tell the CROWler's Engine how to connect to VirusTotal and use its services. VirusTotal will be accessed through CROWler's Detection Rules.",
          "type": "object",
          "properties": {
            "api_key": {
              "title": "VirusTotal API Key",
              "description": "This is the API key that the CROWler will use to connect to VirusTotal.",
              "type": "string"
            }
          },
          "additionalProperties": false,
          "required": [
            "api_key"
          ]
        }
      },
      "additionalProperties": false
    },
    "os": {
      "title": "CROWler (internal) Platform OS Configuration",
      "description": "This is the operating system that the CROWler will use to run. For example, linux, windows or macos. This field is set automatically by the CROWler itself, so no need to set it manually.",
      "type": "string"
    },
    "debug_level": {
      "title": "CROWler Debug Level Configuration",
      "description": "This is the debug level for the CROWler. It is the level of debugging that the CROWler will use to log messages. The higher the level, the more messages will be logged. Don't set or use 0 for NO debug messages.",
      "type": "integer",
      "examples": [
        1
      ]
    }
  },
  "additionalProperties": false,
  "oneOf": [
    {
      "title": "Remote Configuration Mode",
      "description": "Configuration where the 'remote' field must be provided. In this case, the local configuration sections like 'database', 'crawler', and others should not be populated.",
      "allOf": [
        {
          "$ref": "#/properties/remote"
        },
        {
          "title": "Remote Configuration Mode requirements",
          "type": "object",
          "required": [
            "remote"
          ],
          "not": {
            "title": "Local Configuration exclusions",
            "required": [
              "database",
              "crawler",
              "api",
              "selenium",
              "network_info"
            ]
          }
        }
      ]
    },
    {
      "title": "Local Configuration Mode",
      "description": "Configuration where the 'database', 'crawler', 'api', 'selenium', and 'network_info' fields must be provided. In this case, the 'remote' field should not be populated.",
      "type": "object",
      "properties": {
        "database": {
          "$ref": "#/properties/database"
        },
        "crawler": {
          "$ref": "#/properties/crawler"
        },
        "api": {
          "$ref": "#/properties/api"
        },
        "selenium": {
          "$ref": "#/properties/selenium"
        },
        "network_info": {
          "$ref": "#/properties/network_info"
        }
      },
      "required": [
        "database",
        "crawler",
        "api",
        "selenium",
        "network_info"
      ],
      "not": {
        "title": "Local Configuration Mode exclusions",
        "required": [
          "remote"
        ]
      }
    }
  ],
  "dependencies": {
    "database": {
      "not": {
        "required": [
          "remote"
        ]
      }
    },
    "remote": {
      "not": {
        "required": [
          "database",
          "crawler",
          "api",
          "selenium",
          "network_info"
        ]
      }
    }
  }
}
